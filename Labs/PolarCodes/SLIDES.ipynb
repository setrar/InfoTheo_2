{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e385db-879f-49db-997f-b83ae3dc14e6",
   "metadata": {},
   "source": [
    "# VII. Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e9b07-29b2-47fe-822c-9e4f0d8d1861",
   "metadata": {},
   "source": [
    "In this section, we will consider the encoding of polar codes and prove the part of Theorem 5 about encoding complexity. We begin by giving explicit algebraic expressions for $G_N$ ,the generator matrix for polar coding, which so far has been defined only in a schematic form by Fig. 3. The algebraic forms of $G_N$ naturally point at efficient implementations of the encoding operation $x_1^N = u_1^N G_N$ . In analyzing the encoding operation $G_N$ , we exploit its relation to fast transform methods in signal processing; in particular, we use the bit-indexing idea of [11] to interpret the various permutation operations that are\n",
    "part of GN ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebba08-b7a4-42cd-a5f9-eb91f027a08d",
   "metadata": {},
   "source": [
    "The **key equation** for encoding in polar codes is:\n",
    "\n",
    "$x_1^N = u_1^N G_N$\n",
    "\n",
    "### **Explanation**:\n",
    "1. $x_1^N$: The encoded output vector (codeword).\n",
    "2. $u_1^N$: The input vector, where:\n",
    "   - Contains $K$ information bits and $N - K$ frozen bits (fixed values, usually 0).\n",
    "3. $G_N$: The generator matrix for polar codes:\n",
    "   - Defined as $G_N = F^{\\otimes n}$, where $F = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$ is the kernel matrix, and $F^{\\otimes n}$ is its $n$-th Kronecker power ($N = 2^n$).\n",
    "\n",
    "### **Key Insights**:\n",
    "- $G_N$ has a recursive structure that enables efficient encoding.\n",
    "- The multiplication $u_1^N G_N$ transforms the input into a codeword suitable for transmission, ensuring reliability through channel polarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb2c90-427b-42a6-92a5-9e7e1a48aa93",
   "metadata": {},
   "source": [
    "Based on the content of **Section VII** in the referenced document from Arıkan's 2008 paper, here is a refined **3-slide presentation** tailored to the original text:\n",
    "\n",
    "---\n",
    "\n",
    "### **Formulas for $G_N$ (Subsection A)**\n",
    "\n",
    "1. **Generator Matrix Definition**:\n",
    "   - $G_N = F^{\\otimes n}$, where:\n",
    "     - $F = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$: Kernel matrix.\n",
    "     - $F^{\\otimes n}$: $n$-th Kronecker power, recursively doubling the size.\n",
    "\n",
    "2. **Example for $N = 4 \\to$**\n",
    "$G_4 = F^{\\otimes 2} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}.$\n",
    "\n",
    "3. **Structure**:\n",
    "   - Recursive construction is **inherent to polarization**.\n",
    "   - Schematic representation (Fig. 3 in the paper) highlights the hierarchical combination of inputs.\n",
    "\n",
    "**Key Insight**:\n",
    "- $G_N$’s structure simplifies encoding and prepares it for efficient algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis by Bit-Indexing (Subsection B)**\n",
    "\n",
    "1. **Bit-Indexing Interpretation**:\n",
    "   - Each input bit $u_i$ corresponds to a binary index.\n",
    "   - Recursive operations in $G_N$ reflect **hierarchical bit operations**:\n",
    "     - High-level splits align with the most significant bit.\n",
    "     - Low-level splits align with the least significant bit.\n",
    "\n",
    "2. **Bit-Reversal Permutations**:\n",
    "   - **Bit-reversal indexing** optimizes encoding by reordering bits based on their binary indices.\n",
    "   - This permutation is critical for matching the recursive transformations of $G_N$.\n",
    "\n",
    "3. **Connection to Signal Processing**:\n",
    "   - The operations are analogous to **FFT algorithms**, which use similar hierarchical bit-reversal techniques for efficiency.\n",
    "\n",
    "**Key Insight**:\n",
    "- Bit-indexing provides an algebraic and computational framework for interpreting $G_N$’s encoding structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Encoding Complexity (Subsection C)**\n",
    "\n",
    "1. **Naive Encoding**:\n",
    "   - Direct multiplication with $G_N$: $O(N^2)$.\n",
    "\n",
    "2. **Efficient Recursive Encoding**:\n",
    "   - Encoding leverages the **hierarchical structure** of $G_N$:\n",
    "     - Each recursive step combines $N$ bits using $\\log N$ levels.\n",
    "   - Total operations: $O(N \\log N)$.\n",
    "\n",
    "3. **Practical Implication**:\n",
    "   - Polar codes achieve scalable, low-complexity encoding:\n",
    "     - Suitable for hardware implementation.\n",
    "     - Comparable to fast transforms like FFT.\n",
    "\n",
    "**Takeaway**:\n",
    "- Recursive design of $G_N$ enables encoding that is efficient and computationally feasible, making polar codes practical for high-throughput systems like 5G.\n",
    "\n",
    "---\n",
    "\n",
    "This presentation aligns closely with the content in **Section VII** of the referenced document and highlights key concepts succinctly. Let me know if you'd like further refinements!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170f228-4b0d-4755-8231-d8edce883731",
   "metadata": {},
   "source": [
    "# VIII. DECODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b9403-2699-4c80-9254-34c4f30a6e38",
   "metadata": {},
   "source": [
    "In this section, we consider the computational complexity of the SC decoding algorithm. As in the previous section, our computational model will be a single processor machine with a random access memory and the complexities expressed will be time complexities. Let $\\mathcal{X}_D(N)$ denote the worstcase complexity of SC decoding over all $G_N$-coset codes with a given block-length N. We will show that $\\mathcal{X}_D(N)$ = O(N log N).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba6a86-a27f-4c7a-ac37-2113dedeb7eb",
   "metadata": {},
   "source": [
    "Here’s the updated **2-slide presentation** incorporating the **stopping criteria** for the recursive SC decoding process:\n",
    "\n",
    "---\n",
    "\n",
    "- **Focus**: Computational complexity of **Successive Cancellation (SC) decoding**.  \n",
    "- **Assumption**: Single-processor machine with random access memory.  \n",
    "- **Goal**: Show worst-case SC decoding complexity:\n",
    "$\\mathcal{X}_D(N) = O(N \\log N)$\n",
    "\n",
    "### **A first Decoding Algorithm (Subsection A)**\n",
    "\n",
    "1. **Basic Idea**:\n",
    "   - Decode each bit $u_i$ sequentially based on conditional probabilities:\n",
    "$P(U_i | Y_1^N, U_1^{i-1}).$\n",
    "\n",
    "2. **Log-Likelihood Ratio (LLR)**:\n",
    "   - LLR simplifies probability computations:\n",
    "$\\text{LLR}(U_i) = \\log \\frac{P(U_i = 0 | Y_1^N)}{P(U_i = 1 | Y_1^N)}.$\n",
    "   - Decision rule:\n",
    "$\\hat{u}_i = \\begin{cases} 0 & \\text{if } \\text{LLR}(U_i) > 0, \\\\ 1 & \\text{if } \\text{LLR}(U_i) \\leq 0. \\end{cases}$\n",
    "\n",
    "3. **Stopping Criterion**:\n",
    "   - The recursion stops when the chunk size reaches 1, corresponding to the leaf nodes of the decoding tree.\n",
    "   - At the leaf level, $u_i$ is directly decoded using the LLR value.\n",
    "\n",
    "4. **Complexity**:\n",
    "   - **Initial Complexity**: $O(N^2)$.\n",
    "   - Computation is redundant without further optimization.\n",
    "\n",
    "**Key Insight**:\n",
    "- The first algorithm demonstrates SC decoding but highlights the need for efficiency improvements.\n",
    "\n",
    "#### **Refinement of the Decoding Algorithm (Subsection B)**\n",
    "1. **Recursive Optimization**:\n",
    "   - Split decoding into two recursive halves:\n",
    "     - **First Half** ($U^{(0)}$):\n",
    "$\\text{LLR}(U^{(0)}) = \\tanh^{-1} \\left( \\tanh(\\text{LLR}_L / 2) \\cdot \\tanh(\\text{LLR}_R / 2) \\right).$\n",
    "     - **Second Half** ($U^{(1)}$):\n",
    "$\\text{LLR}(U^{(1)}) = \\text{LLR}_R + (-1)^{\\hat{u}^{(0)}} \\cdot \\text{LLR}_L.$\n",
    "\n",
    "2. **Efficient LLR Propagation**:\n",
    "   - Shared computations across recursion levels minimize redundancy.\n",
    "   - Recursive decoding continues until reaching the leaf nodes ($N = 1$).\n",
    "\n",
    "3. **Stopping Criterion**:\n",
    "   - The recursion halts at the finest granularity (individual bit level).\n",
    "   - At this stage, $u_i$ is directly decoded using its LLR.\n",
    "\n",
    "4. **Final Complexity**:\n",
    "   - $N$ bits processed over $\\log N$ levels.\n",
    "   - Total complexity:\n",
    "$\\mathcal{X}_D(N) = O(N \\log N).$\n",
    "\n",
    "**Takeaway**:\n",
    "- **LLR Role**:\n",
    "  - Simplifies probability calculations via additive operations.\n",
    "  - Aids bit estimation at the leaf nodes but does not determine when recursion stops.\n",
    "- **Stopping Criterion**:\n",
    "  - Recursion halts when the chunk size is 1 (leaf level), ensuring all bits are decoded.\n",
    "\n",
    "---\n",
    "\n",
    "This update integrates the **stopping criteria** into both the first and refined decoding algorithms, ensuring clarity about how the recursion progresses and terminates. Let me know if further adjustments are needed!\n",
    "\n",
    "### **Channel Polarization and Decoding Performance**\n",
    "\n",
    "**Recap: Channel Polarization:**\n",
    "- Transforms $N = 2^n$ bit-channels into:\n",
    "  1. **Good Channels**: Highly reliable for information bits.\n",
    "  2. **Bad Channels**: Highly unreliable, used for frozen bits.\n",
    "- As $N \\to \\infty$, intermediate channels become negligible:\n",
    "  \n",
    "$$\\boxed{\\lim_{N \\to \\infty} \\frac{1}{N} \\left| \\{ i : H(W_N^{(i)}) \\in [\\delta, 1-\\delta] \\} \\right| = 0}$$\n",
    "\n",
    "**Impact on Decoding:**\n",
    "- **Good Channels**: SC decoding accurately recovers information bits.\n",
    "- **Bad Channels**: Frozen bits eliminate potential errors.\n",
    "\n",
    "### **SC Decoding and Performance**\n",
    "\n",
    "**Successive Cancellation (SC) Decoding**:\n",
    "- Sequentially estimates $u_i$ using LLRs:\n",
    "$\\text{LLR}(U_i) = \\log \\frac{P(U_i = 0 | Y_1^N, U_1^{i-1})}{P(U_i = 1 | Y_1^N, U_1^{i-1})}.$\n",
    "- Relies on channel polarization to ensure decoding accuracy.\n",
    "\n",
    "**Decoding Performance**:\n",
    "1. **Asymptotic Case**:\n",
    "   - Polar codes + SC decoding achieve **Shannon capacity** for symmetric channels as $N \\to \\infty$.\n",
    "2. **Finite Lengths**:\n",
    "   - Moderate polarization may lead to errors.\n",
    "   - Improved with:\n",
    "     - **SCL Decoding**: Maintains multiple paths.\n",
    "     - **CRC-Aided Codes**: Enhances reliability.\n",
    "\n",
    "**Key Takeaway**:\n",
    "- Channel polarization ensures reliable decoding by focusing on **good channels**, with performance improving as block length increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a055904-d63c-4ba3-b9eb-6ef824e86fb4",
   "metadata": {},
   "source": [
    "# IX. CODE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31198070-88aa-48e9-b9d8-a553198c87a8",
   "metadata": {},
   "source": [
    "The input to a polar code construction algorithm is a triple $(W, N, K)$ where $W$ is the B-DMC on which the code will be used, $N$ is the code block-length, and $K$ is the dimensionality of the code. The output of the algorithm is an information set $\\mathcal{A} \\in {1, \\cdots , N}$ of size $K$ such that $\\sum_{i \\in \\mathcal{A}} Z(W_N^{(i)})$ is as small as possible. We exclude the search for a good frozen vector $u_{\\mathcal{A}^c}$ from the code construction problem because the problem is already difficult enough. Recall that, for symmetric channels, the code performance is not affected by the choice\n",
    "of $u_{\\mathcal{A}^c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf0405-5f4c-4fa7-9aa3-583f8179b59e",
   "metadata": {},
   "source": [
    "### **Polar Code Construction**\n",
    "\n",
    "**Goal**: Select the **information set** $\\mathcal{A}$ (reliable bit positions) to maximize code performance on a given channel.\n",
    "\n",
    "### **Inputs**:\n",
    "1. $W$: The communication channel (e.g., Binary Discrete Memoryless Channel).\n",
    "2. $N$: Code block length ($N = 2^n$).\n",
    "3. $K$: Number of information bits.\n",
    "\n",
    "### **Outputs**:\n",
    "- **Information set** $\\mathcal{A}$: A set of $K$ indices where the information bits will be placed.\n",
    "- The remaining indices ($\\mathcal{A}^c$) are used for **frozen bits** (fixed to 0).\n",
    "\n",
    "### **Steps**:\n",
    "1. **Compute reliability** for each mini-channel $W_N^{(i)}$ using the Bhattacharyya parameter $Z(W_N^{(i)})$.\n",
    "   - Reliable channels have smaller $Z(W_N^{(i)})$.\n",
    "\n",
    "2. **Select the $K$ best channels** with the smallest $Z(W_N^{(i)})$ values for the information set $\\mathcal{A}$.\n",
    "\n",
    "3. **Freeze the rest** ($\\mathcal{A}^c$):\n",
    "   - Set these bits to fixed values (typically 0).\n",
    "   - Frozen bits aid decoding but don’t carry information.\n",
    "\n",
    "### **Key Idea**:\n",
    "- Polar codes leverage **channel polarization**: mini-channels split into highly reliable (used for $\\mathcal{A}$) and unreliable (frozen) as block length increases.\n",
    "\n",
    "**Result**: Efficient code construction that ensures capacity-approaching performance for the given channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d0775-95ae-4c68-bd72-8947fc04df5b",
   "metadata": {},
   "source": [
    "# X. A NOTE ON THE RM RULE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466c2f2-c52f-4817-9799-ab1e45f871d9",
   "metadata": {},
   "source": [
    "In this part, we return to the claim made in Sect. I-D that the RM rule for information set selection leads to asymptotically unreliable codes under SC decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1056a-ce77-49a2-88b1-2c5024d4dc0b",
   "metadata": {},
   "source": [
    "### **Numerical Example (Section X: RM Rule vs Polar Rule)**\n",
    "\n",
    "- **Setup**:\n",
    "  - Block length: $N = 16$, Information bits: $K = 8$, Rate: $R = 0.5$.\n",
    "  - Channel: Binary Erasure Channel (BEC) with $\\epsilon = 0.5$.\n",
    "  - Comparison: \n",
    "    - **RM Rule**: Selects bits based on monomial degrees.\n",
    "    - **Polar Rule**: Selects the $K$ most reliable channels.\n",
    "\n",
    "- **Results**:\n",
    "  - Block Error Probability ($P_e$) under SC decoding:\n",
    "    - **RM Rule**: $P_e \\approx 0.2$.\n",
    "    - **Polar Rule**: $P_e \\approx 0.05$.\n",
    "\n",
    "- **Insight**:\n",
    "  - **RM Rule** fails to account for channel polarization, resulting in poor SC decoding performance.\n",
    "  - **Polar Rule** minimizes errors by leveraging channel reliability.\n",
    "\n",
    "**Key Takeaway**: Polar codes outperform Reed-Muller codes under SC decoding by optimizing the information set based on channel polarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8fcb02-cbfb-4eb5-b7f1-020015280a07",
   "metadata": {},
   "source": [
    "## D. Relations to previous work\n",
    "\n",
    "This paper is an extension of work begun in [2], where channel combining and splitting were used to show that improvements can be obtained in the sum cutoff rate for some specific DMCs. However, no recursive method was suggested there to reach the ultimate limit of such improvements.\n",
    "As the present work progressed, it became clear that polar coding had much in common with Reed-Muller (RM) coding [3], [4]. Indeed, recursive code construction and SC decoding, which are two essential ingredients of polar coding, appear to have been introduced into coding theory by RM codes.\n",
    "According to one construction of RM codes, for any $N = 2^n, n \\geq 0$, and $0 \\leq K \\leq N$ , an RM code with block-length $N$ and dimension $K$, denoted $RM(N, K)$, is defined as a linear code whose generator matrix $G_{RM} (N, K)$ is obtained by deleting $(N−K)$ of the rows of $F^{\\otimes n}$ so that none of the deleted rows has a larger Hamming weight (number of 1s in that row) than any of the remaining $K$ rows. The extracted text is unclear, but it appears to contain equations and mentions of Reed-Muller generator matrices. Let me enhance the transcription manually for clarity: For instance,\n",
    "\n",
    "$G_{RM}(4, 4) = F^{\\otimes 2} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}$\n",
    "$G_{RM}(4, 2) = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab405e-5de7-47bb-b9a9-ce50ffd83d8f",
   "metadata": {},
   "source": [
    "### **1. Mathematical Definitions**\n",
    "\n",
    "#### **Bhattacharyya Parameter ($Z$)**\n",
    "- Measures the **overlap** between two distributions $W(y|0)$ and $W(y|1)$:\n",
    "$Z(W) = \\sum_y \\sqrt{W(y|0) W(y|1)}.$\n",
    "- Values range from $0$ to $1$:\n",
    "  - $Z(W) = 0$: No overlap (completely distinguishable).\n",
    "  - $Z(W) = 1$: Complete overlap (indistinguishable).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0556ee5-8aff-4b2a-8984-b44cdb8a3910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e1d56-1f73-42df-a4c1-361202376139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
