{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83af5ce4-1bf5-4515-a80f-ac3c9c4df21f",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c11121-4c09-4116-bfdd-081c92cacd92",
   "metadata": {},
   "source": [
    "The unit of **entropy** is the **bit** when the logarithm in the entropy formula is computed using base 2. Here's why:\n",
    "\n",
    "### **Entropy and Information**\n",
    "Entropy quantifies the uncertainty or information content in a random variable. When expressed in bits:\n",
    "- **One bit** represents the amount of information required to distinguish between two equally likely outcomes (e.g., heads or tails in a fair coin flip).\n",
    "\n",
    "### **Logarithm Base 2**\n",
    "The entropy formula is:\n",
    "$\n",
    "H(X) = - \\sum_{x \\in X} P(x) \\log_b P(x)\n",
    "$\n",
    "- The base $ b $ of the logarithm determines the unit of entropy.\n",
    "  - If $ b = 2 $, the unit is **bits**.\n",
    "  - If $ b = e $, the unit is **nats**.\n",
    "  - If $ b = 10 $, the unit is **hartleys**.\n",
    "  \n",
    "Using base 2 means the entropy measures how many binary decisions (yes/no or 0/1) are required to encode or represent the information.\n",
    "\n",
    "### **Bits as a Natural Choice**\n",
    "1. **Binary Systems**: In digital systems, data is stored and processed in binary (0s and 1s), making bits a natural unit for measuring information.\n",
    "2. **Interpretability**: A value of $ H(X) = 3 $ bits, for example, means 3 binary decisions (or a message length of 3 bits) are needed, on average, to describe the random variable $ X $.\n",
    "\n",
    "### **Practical Examples**\n",
    "- A fair coin flip has entropy:\n",
    "  $\n",
    "  H(X) = -\\sum_{x} P(x) \\log_2 P(x) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1 \\text{ bit}.\n",
    "  $\n",
    "  This reflects the need for **1 bit** to encode the result (heads or tails).\n",
    "  \n",
    "Thus, the unit **bit** naturally emerges when dealing with binary systems and information theory applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b2fcb-6717-4cac-9de6-043d5525c9b9",
   "metadata": {},
   "source": [
    "Whether higher entropy (greater uncertainty) is **better or worse** depends on the **context**:\n",
    "\n",
    "---\n",
    "\n",
    "### **When Higher Entropy is Better:**\n",
    "1. **Cryptography**:\n",
    "   - Higher entropy in cryptographic keys or random number generators means greater randomness and security.\n",
    "   - Example: A cryptographic key with higher entropy is harder to guess or brute-force.\n",
    "\n",
    "2. **Diversity and Fairness**:\n",
    "   - In resource distribution or decision-making, higher entropy may indicate greater diversity or fairness.\n",
    "   - Example: A perfectly fair dice roll has maximum entropy.\n",
    "\n",
    "3. **Unbiased Information**:\n",
    "   - High entropy often indicates unbiased and unpredictable outcomes, desirable in simulations or randomized experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### **When Lower Entropy is Better:**\n",
    "1. **Compression**:\n",
    "   - Lower entropy means less uncertainty, enabling more efficient data compression.\n",
    "   - Example: Repeated or predictable patterns (low entropy) are easier to compress.\n",
    "\n",
    "2. **Signal Clarity**:\n",
    "   - In communication systems, low entropy in the signal helps reduce noise and improve reliability.\n",
    "   - Example: A predictable signal is easier to decode.\n",
    "\n",
    "3. **Control and Predictability**:\n",
    "   - In deterministic systems, low entropy signifies more control or predictability.\n",
    "   - Example: In production systems, less variability (low entropy) means consistent quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**:\n",
    "- **Higher entropy**: Desirable for randomness, fairness, and security.\n",
    "- **Lower entropy**: Desirable for predictability, efficiency, and control.\n",
    "\n",
    "It depends on whether the situation benefits from uncertainty (e.g., randomness in security) or order (e.g., precision in communication)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3771025-d176-44ee-a289-982f7893690b",
   "metadata": {},
   "source": [
    "The difference between $ H(.) $ and $ I(.) $ lies in what they measure in information theory:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Entropy ($ H(.) $)**\n",
    "- **Definition**: $ H(X) $ quantifies the uncertainty or the average amount of information in a random variable $ X $.\n",
    "- **Formula**:\n",
    "  $\n",
    "  H(X) = -\\sum_{x \\in X} P(x) \\log P(x)\n",
    "  $\n",
    "- **Meaning**:\n",
    "  - Measures **uncertainty**: Higher entropy means more unpredictability.\n",
    "  - If $ X $ has equally likely outcomes, $ H(X) $ is maximized.\n",
    "  - Unit: Bits (if logarithm base 2).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Mutual Information ($ I(.) $)**\n",
    "- **Definition**: $ I(X; Y) $ quantifies the amount of information that one random variable $ X $ provides about another variable $ Y $. It measures the reduction in uncertainty about $ Y $ given knowledge of $ X $.\n",
    "- **Formula**:\n",
    "  $\n",
    "  I(X; Y) = H(X) + H(Y) - H(X, Y)\n",
    "  $\n",
    "  or equivalently:\n",
    "  $\n",
    "  I(X; Y) = H(Y) - H(Y|X)\n",
    "  $\n",
    "- **Meaning**:\n",
    "  - Measures **shared information** between $ X $ and $ Y $.\n",
    "  - $ I(X; Y) = 0 $ if $ X $ and $ Y $ are independent (no shared information).\n",
    "  - $ I(X; Y) = H(X) = H(Y) $ if $ X $ completely determines $ Y $ (and vice versa).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**\n",
    "| Aspect              | $ H(.) $: Entropy                      | $ I(.) $: Mutual Information           |\n",
    "|---------------------|------------------------------------------|------------------------------------------|\n",
    "| **Measures**        | Uncertainty in a single random variable  | Shared information between two variables |\n",
    "| **Purpose**         | Quantifies unpredictability              | Quantifies dependency                    |\n",
    "| **Range**           | $ H(X) \\geq 0 $                       | $ I(X; Y) \\geq 0 $                    |\n",
    "| **Context**         | Used to evaluate randomness              | Used to measure relationships            |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "- $ H(X) $: Measures the uncertainty in the outcome of rolling a die (e.g., $ \\log_2(6) $ for a fair die).\n",
    "- $ I(X; Y) $: Measures how much information knowing $ X $ (e.g., a biased die roll) reduces uncertainty about $ Y $ (e.g., the sum of two rolls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b579cd-bc77-4d06-afaa-9d4af979e5ff",
   "metadata": {},
   "source": [
    "In **Information Theory**, the expression $ E[\\mathcal{L}(\\xi(X^n))] $ often represents the **expected distortion** or **expected cost** when encoding or processing a sequence $ X^n $:\n",
    "\n",
    "$\n",
    "E[\\mathcal{L}(\\xi(X^n))] = \\text{Expected distortion or loss when using } \\xi \\text{ as the encoder/estimator}.\n",
    "$\n",
    "\n",
    "- **$ \\xi(X^n) $**: The encoding or decision rule based on $ X^n $.\n",
    "- **$ \\mathcal{L} $**: A distortion or loss function, such as squared error or log loss.\n",
    "- **Application**: Captures the trade-off between compression (e.g., rate) and fidelity (e.g., distortion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f68300-b9c1-413e-ad64-0adbba38f195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
