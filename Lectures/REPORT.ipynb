{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8820b0d1-a3d9-4f76-906c-d1537e51b832",
   "metadata": {},
   "source": [
    "# Information Theory CheatSheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c82a28-bd87-45f3-a116-3425bf9a5804",
   "metadata": {},
   "source": [
    "## Quick Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e67948-c314-4e1c-acc9-a053dcd608bc",
   "metadata": {},
   "source": [
    "### **Information Theory CheatSheet**  \n",
    "*(Based on Elements of Information Theory, 2nd Edition by Thomas M. Cover, Joy A. Thomas)*  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Capacity Regions**  \n",
    "- **Multiple Access Channel (MAC):**  \n",
    "  Capacity region:  \n",
    "$R_1 \\leq I(X_1; Y | X_2), \\quad R_2 \\leq I(X_2; Y | X_1), \\quad R_1 + R_2 \\leq I(X_1, X_2; Y)$\n",
    "\n",
    "- **Broadcast Channel:**  \n",
    "  - No general formula for all cases.  \n",
    "  - For **degraded channels**, optimal rates achieved using **superposition coding**:  \n",
    "$R_1 \\leq I(X; Y_1), \\quad R_2 \\leq I(X; Y_2 | Y_1)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Markov Chains**  \n",
    "- **Definition:**  \n",
    "  A stochastic process where future states depend only on the current state:  \n",
    "$P(X_{n+1} | X_n, X_{n-1}, \\dots) = P(X_{n+1} | X_n)$\n",
    "\n",
    "- **Entropy Rate:**  \n",
    "$H(X) = \\lim_{n \\to \\infty} \\frac{H(X_1, X_2, \\dots, X_n)}{n}$\n",
    "\n",
    "- **Stationary Distribution:**  \n",
    "  For transition matrix $P$, solve $\\pi P = \\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Maximization of Entropy**  \n",
    "- **Discrete case:**  \n",
    "  Entropy is maximized when all outcomes are equally likely:  \n",
    "$H(X) \\leq \\log_2 |\\mathcal{X}|$\n",
    "\n",
    "- **Continuous case:**  \n",
    "  Differential entropy is maximized by a Gaussian distribution:  \n",
    "$h(X) \\leq \\frac{1}{2} \\log_2 (2 \\pi e \\sigma^2)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Capacities of Different Channels**  \n",
    "1. **Binary Symmetric Channel (BSC):**  \n",
    "$C = 1 - H(p), \\quad H(p) = -p \\log_2 p - (1-p) \\log_2 (1-p)$\n",
    "\n",
    "2. **Binary Erasure Channel (BEC):**  \n",
    "$C = 1 - p$\n",
    "\n",
    "3. **AWGN Channel:**  \n",
    "$C = \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P}{N_0 B} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Calculate Entropy of Channels**  \n",
    "- **Mutual Information:**  \n",
    "$I(X; Y) = H(Y) - H(Y | X)$\n",
    "\n",
    "- **Entropy of a channel with output $Y$:**  \n",
    "$H(Y) = -\\sum_{y \\in \\mathcal{Y}} P(y) \\log_2 P(y)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Index Coding**  \n",
    "- **Definition:**  \n",
    "  Reduce the number of transmissions by using side information at clients.  \n",
    "\n",
    "- **Example:**  \n",
    "  For messages $W_1, W_2, W_3$ and side information:  \n",
    "  - Client 1 knows $W_2$  \n",
    "  - Client 2 knows $W_3$  \n",
    "  - Client 3 knows $W_1$  \n",
    "  Optimal coded transmissions: $W_1 \\oplus W_2, W_2 \\oplus W_3, W_3 \\oplus W_1$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Network Coding**  \n",
    "- **Definition:**  \n",
    "  Intermediate nodes perform operations (e.g., XOR) on data streams to increase throughput.\n",
    "\n",
    "- **Example:**  \n",
    "  In a butterfly network, transmit $X = A \\oplus B$. Both sinks decode:  \n",
    "$A = X \\oplus B, \\quad B = X \\oplus A$\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Coded Caching**  \n",
    "- **Basic Idea:**  \n",
    "  Pre-store coded data at users to reduce peak-time traffic.\n",
    "\n",
    "- **Formula:**  \n",
    "$L = \\frac{N(1 - M/N)}{1 + KM/N}$\n",
    "  where $N$ is the number of files, $M$ is the cache size per user, and $K$ is the number of users.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Gambling (after 10 goals)**  \n",
    "- **Kelly Criterion:**  \n",
    "  Maximizes logarithmic utility by choosing the optimal bet fraction:  \n",
    "$f^* = \\frac{bp - q}{b}, \\quad q = 1 - p$\n",
    "\n",
    "- **Example:**  \n",
    "  If $p = 0.6$ and odds $b = 2$, the optimal bet is:  \n",
    "$f^* = \\frac{2 \\cdot 0.6 - 0.4}{2} = 0.4$\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. MAC or Broadcast Channel (Optimal Schemes)**  \n",
    "- **MAC:**  \n",
    "  Achieve optimal rates using **successive interference cancellation**:  \n",
    "$R_1 \\leq I(X_1; Y | X_2), \\quad R_2 \\leq I(X_2; Y | X_1)$\n",
    "\n",
    "- **Broadcast:**  \n",
    "  Achieve capacity using **superposition coding**:  \n",
    "$X = \\alpha X_1 + (1 - \\alpha) X_2$\n",
    "\n",
    "---\n",
    "\n",
    "#### **11. Asymptotic Equipartition Property (AEP)**  \n",
    "- **Definition:**  \n",
    "  For a sequence of i.i.d. random variables, the probability of typical sequences converges to:  \n",
    "$P(x^n) \\approx 2^{-nH(X)}$\n",
    "\n",
    "- **Implications:**  \n",
    "  - Most sequences are typical as $n \\to \\infty$.  \n",
    "  - Supports **data compression** and **channel coding** by focusing on typical sequences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12. Coded MapReduce**  \n",
    "- **Definition:**  \n",
    "  Encode intermediate data to reduce communication during the shuffle phase.\n",
    "\n",
    "- **Example:**  \n",
    "  If there are 4 mappers and 3 reducers, coded transmissions allow each reducer to decode its required data from fewer transmissions.\n",
    "\n",
    "- **Communication Reduction:**  \n",
    "$R = \\frac{1}{r}$\n",
    "  where $r$ is the number of reducers.\n",
    "\n",
    "---\n",
    "\n",
    "This cheat sheet covers essential formulas, examples, and definitions for each topic, providing a quick reference for **Information Theory** concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026e56-c576-4bb1-b8a9-a236320cbe50",
   "metadata": {},
   "source": [
    "## Exercices 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1601e0-440c-4d9a-810f-90bb81a24bec",
   "metadata": {},
   "source": [
    "### **Information Theory Q&A with Mathematical Problems**  \n",
    "*(With focus on AEP and related concepts)*  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Capacity Regions**  \n",
    "1. **Q:**  \n",
    "   For a two-user Gaussian multiple access channel (MAC) with $P_1 = 4$, $P_2 = 6$, and noise $N = 2$, find the sum-rate constraint.  \n",
    "   **A:**  \n",
    "   The sum-rate constraint is:  \n",
    "$R_1 + R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1 + P_2}{N} \\right) = \\frac{1}{2} \\log_2 \\left( 1 + \\frac{4 + 6}{2} \\right) = 1.8 \\, \\text{bits}$\n",
    "\n",
    "2. **Q:**  \n",
    "   Explain how the capacity region changes when time-sharing is used in a broadcast channel.  \n",
    "   **A:**  \n",
    "   Time-sharing allows convex combinations of achievable rate points, expanding the capacity region by alternating between different transmission schemes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Markov Chains**  \n",
    "1. **Q:**  \n",
    "   For a Markov chain with transition matrix  \n",
    "$P = \\begin{bmatrix} 0.6 & 0.4 \\\\ 0.3 & 0.7 \\end{bmatrix},$\n",
    "   find the stationary distribution.  \n",
    "   **A:**  \n",
    "   Solve $\\pi P = \\pi$ with $\\pi_1 + \\pi_2 = 1$:  \n",
    "$\\pi_1 = 0.6\\pi_1 + 0.3\\pi_2, \\quad \\pi_2 = 0.4\\pi_1 + 0.7\\pi_2$\n",
    "   Solution: $\\pi = (0.43, 0.57)$.\n",
    "\n",
    "2. **Q:**  \n",
    "   Calculate the entropy rate of this Markov chain.  \n",
    "   **A:**  \n",
    "$H(X) = \\sum_{i, j} \\pi(i) P_{ij} \\log_2 \\frac{1}{P_{ij}}$\n",
    "   Substituting values:  \n",
    "$H(X) = 0.43 \\cdot (0.6 \\log_2 \\frac{1}{0.6} + 0.4 \\log_2 \\frac{1}{0.4}) + 0.57 \\cdot (0.3 \\log_2 \\frac{1}{0.3} + 0.7 \\log_2 \\frac{1}{0.7})$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Maximization of Entropy**  \n",
    "1. **Q:**  \n",
    "   Prove that entropy is maximized for a discrete variable when all outcomes are equally likely.  \n",
    "   **A:**  \n",
    "   If $p(x) = \\frac{1}{|\\mathcal{X}|}$, then:  \n",
    "$H(X) = -\\sum_{x \\in \\mathcal{X}} \\frac{1}{|\\mathcal{X}|} \\log_2 \\frac{1}{|\\mathcal{X}|} = \\log_2 |\\mathcal{X}|$\n",
    "\n",
    "2. **Q:**  \n",
    "   Calculate the differential entropy of a Gaussian variable with variance $\\sigma^2 = 3$.  \n",
    "   **A:**  \n",
    "$h(X) = \\frac{1}{2} \\log_2 (2 \\pi e \\sigma^2) = \\frac{1}{2} \\log_2 (2 \\pi e \\cdot 3) \\approx 2.77 \\, \\text{bits}$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Capacities of Different Channels**  \n",
    "1. **Q:**  \n",
    "   For a binary symmetric channel (BSC) with $p = 0.2$, calculate the channel capacity.  \n",
    "   **A:**  \n",
    "$C = 1 - H(p), \\quad H(p) = -p \\log_2 p - (1-p) \\log_2 (1-p)$\n",
    "   Substitution gives $H(p) \\approx 0.72$, so $C \\approx 0.28 \\, \\text{bits}$.\n",
    "\n",
    "2. **Q:**  \n",
    "   Find the capacity of an AWGN channel with power $P = 10$, noise spectral density $N_0 = 1$, and bandwidth $B = 1$.  \n",
    "   **A:**  \n",
    "$C = \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P}{N_0 B} \\right) = \\frac{1}{2} \\log_2 (1 + 10) = 1.73 \\, \\text{bits}$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Calculate Entropy of Channels**  \n",
    "1. **Q:**  \n",
    "   Given $P(Y=1|X=1) = 0.9$ and $P(Y=0|X=0) = 0.8$, find the conditional entropy $H(Y|X)$.  \n",
    "   **A:**  \n",
    "$H(Y | X) = 0.5 \\left( -0.9 \\log_2 0.9 - 0.1 \\log_2 0.1 \\right) + 0.5 \\left( -0.8 \\log_2 0.8 - 0.2 \\log_2 0.2 \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Index Coding**  \n",
    "1. **Q:**  \n",
    "   For messages $W_1, W_2, W_3$, find the optimal index code if:  \n",
    "   - Client 1 knows $W_2$,  \n",
    "   - Client 2 knows $W_3$,  \n",
    "   - Client 3 knows $W_1$.  \n",
    "   **A:**  \n",
    "   Coded transmissions: $W_1 \\oplus W_2, W_2 \\oplus W_3, W_3 \\oplus W_1$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Network Coding**  \n",
    "1. **Q:**  \n",
    "   In a butterfly network, compute the coded message if $A = 1$ and $B = 0$.  \n",
    "   **A:**  \n",
    "   Transmit $X = A \\oplus B = 1$. Both sinks decode:  \n",
    "$A = X \\oplus B = 1, \\quad B = X \\oplus A = 0$\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Coded Caching**  \n",
    "1. **Q:**  \n",
    "   For $N = 4$, $K = 2$, and $M = 1$, find the communication load.  \n",
    "   **A:**  \n",
    "$L = \\frac{N(1 - M/N)}{1 + KM/N} = \\frac{4(1 - 1/4)}{1 + 2(1/4)} = 2.4$\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Gambling (after 10 Gains)**  \n",
    "\n",
    "1. **Q:**  \n",
    "   Suppose you have achieved 10 consecutive gains and your current wealth is $W = 1000$. The probability of winning the next bet is $p = 0.55$ and the odds are $b = 2$. Apply the **Kelly Criterion** to determine the optimal bet size.\n",
    "\n",
    "   **A:**  \n",
    "   The **Kelly Criterion** formula is:  \n",
    "$f^* = \\frac{bp - (1-p)}{b}$\n",
    "\n",
    "   Substituting values:  \n",
    "$f^* = \\frac{2 \\cdot 0.55 - 0.45}{2} = \\frac{1.1 - 0.45}{2} = 0.325$\n",
    "\n",
    "   The optimal bet size is $32.5\\%$ of your current wealth:  \n",
    "$f^* \\cdot W = 0.325 \\cdot 1000 = 325$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. MAC or Broadcast Channel (Optimal Schemes)**  \n",
    "1. **Q:**  \n",
    "   For a MAC with $P_1 = 3$, $P_2 = 5$, and noise $N = 1$, find the individual rates.  \n",
    "   **A:**  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1}{N} \\right) = 1, \\quad R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_2}{N} \\right) = 1.32$\n",
    "\n",
    "---\n",
    "\n",
    "#### **11. Asymptotic Equipartition Property (AEP)**  – Picking a Small Subset of Numbers Problem**\n",
    "\n",
    "#### **Q:**  \n",
    "Given a random variable $X$ with entropy $H(X) = 2 \\, \\text{bits}$, there are $2^{10} = 1024$ possible sequences of length $n = 5$. You want to find a small subset of sequences such that their total probability is at least $0.99$. How many sequences should you pick from the typical set?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Solution:**\n",
    "\n",
    "1. **Typical Set Definition:**  \n",
    "   The **typical set** $A_\\epsilon^{(n)}$ contains sequences $x^n$ whose probability is approximately:\n",
    "$P(x^n) \\approx 2^{-nH(X)} = 2^{-5 \\cdot 2} = 2^{-10}$\n",
    "\n",
    "2. **Total Number of Typical Sequences:**  \n",
    "   The number of sequences in the typical set is approximately:\n",
    "$|A_\\epsilon^{(n)}| \\approx 2^{nH(X)} = 2^{10} = 1024$\n",
    "\n",
    "3. **Finding the Required Subset:**  \n",
    "   To achieve a cumulative probability of at least $0.99$, we need the smallest number $m$ of sequences such that:\n",
    "$m \\cdot 2^{-10} \\geq 0.99$\n",
    "\n",
    "   Solving for $m$:\n",
    "$m \\geq \\frac{0.99}{2^{-10}} = 0.99 \\cdot 1024 = 1013$\n",
    "\n",
    "4. **Answer:**  \n",
    "   You need to pick at least **1013 sequences** from the typical set to ensure a cumulative probability of at least **0.99**.\n",
    "\n",
    "This problem demonstrates how AEP helps determine the number of typical sequences necessary to capture most of the probability mass.\n",
    "\n",
    "---\n",
    "\n",
    "#### **12. Coded MapReduce**  \n",
    "1. **Q:**  \n",
    "   For 4 mappers and 3 reducers, calculate the communication reduction using coded MapReduce.  \n",
    "   **A:**  \n",
    "$R = \\frac{1}{r} = \\frac{1}{3}$\n",
    "\n",
    "---\n",
    "\n",
    "This set of Q&As is designed to test both conceptual understanding and mathematical problem-solving skills in **Information Theory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858cd37-7e4c-4c01-bff5-195b7a9bb025",
   "metadata": {},
   "source": [
    "## Exercices 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d29af-2947-4c8f-bce4-05147bd7f0c3",
   "metadata": {},
   "source": [
    "Here’s a new **set of advanced questions and answers** covering all the topics on the list, focusing on more difficult mathematical problems.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Capacity Regions**  \n",
    "1. **Q:**  \n",
    "   Consider a two-user Gaussian multiple access channel (MAC) with $P_1 = 3$, $P_2 = 5$, and noise $N = 2$. Find all valid rate pairs $(R_1, R_2)$.  \n",
    "\n",
    "   **A:**  \n",
    "   The constraints are:  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1}{N} \\right), \\quad R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_2}{N} \\right), \\quad R_1 + R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1 + P_2}{N} \\right)$\n",
    "\n",
    "   Calculating:  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2(1 + 1.5) \\approx 0.58, \\quad R_2 \\leq \\frac{1}{2} \\log_2(1 + 2.5) \\approx 0.92$\n",
    "$R_1 + R_2 \\leq \\frac{1}{2} \\log_2(1 + 4) \\approx 1.16$\n",
    "\n",
    "   The capacity region consists of all rate pairs that satisfy these inequalities.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Markov Chains**  \n",
    "1. **Q:**  \n",
    "   A Markov chain has the following transition matrix:  \n",
    "$P = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.3 & 0.7 \\end{bmatrix}$\n",
    "   Find the stationary distribution and the entropy rate.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Find the stationary distribution $\\pi$. Solve $\\pi P = \\pi$:  \n",
    "$\\pi_1 = 0.5\\pi_1 + 0.3\\pi_2, \\quad \\pi_2 = 0.5\\pi_1 + 0.7\\pi_2, \\quad \\pi_1 + \\pi_2 = 1$\n",
    "   Solving gives $\\pi = (0.375, 0.625)$.\n",
    "\n",
    "   **Step 2:** Calculate entropy rate:  \n",
    "$H(X) = \\sum_{i, j} \\pi(i) P_{ij} \\log_2 \\frac{1}{P_{ij}}$\n",
    "   Substitution yields the entropy rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Maximization of Entropy**  \n",
    "1. **Q:**  \n",
    "   A continuous random variable $X$ has a Gaussian distribution with variance $\\sigma^2 = 4$. Find its differential entropy and compare it to the maximum entropy of a uniform distribution over the interval $[-a, a]$.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Differential entropy of Gaussian:  \n",
    "$h(X) = \\frac{1}{2} \\log_2 (2 \\pi e \\sigma^2) = \\frac{1}{2} \\log_2 (2 \\pi e \\cdot 4) \\approx 3.06 \\, \\text{bits}$\n",
    "\n",
    "   **Step 2:** For a uniform distribution:  \n",
    "$h(X) = \\log_2 (2a)$\n",
    "   To match the variance of the Gaussian, $a = 2\\sqrt{3}$, so $h(X) = \\log_2 (4\\sqrt{3}) \\approx 3.17 \\, \\text{bits}$.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Capacities of Different Channels**  \n",
    "1. **Q:**  \n",
    "   Calculate the capacity of a binary symmetric channel (BSC) with crossover probability $p = 0.3$.\n",
    "\n",
    "   **A:**  \n",
    "$C = 1 - H(p), \\quad H(p) = -p \\log_2 p - (1-p) \\log_2 (1-p)$\n",
    "   Substituting $p = 0.3$:  \n",
    "$H(0.3) = -(0.3 \\log_2 0.3 + 0.7 \\log_2 0.7) \\approx 0.881$\n",
    "$C = 1 - 0.881 = 0.119 \\, \\text{bits}$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Calculate Entropy of Channels**  \n",
    "1. **Q:**  \n",
    "   For a channel with transition matrix:  \n",
    "$P(Y|X) = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.2 & 0.8 \\end{bmatrix},$\n",
    "   and input probabilities $P(X=1) = 0.6$, find the mutual information $I(X; Y)$.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Find $P(Y)$:  \n",
    "$P(Y=1) = 0.6 \\cdot 0.9 + 0.4 \\cdot 0.2 = 0.62, \\quad P(Y=2) = 0.6 \\cdot 0.1 + 0.4 \\cdot 0.8 = 0.38$\n",
    "\n",
    "   **Step 2:** Calculate $H(Y)$ and $H(Y|X)$.  \n",
    "$H(Y) = - (0.62 \\log_2 0.62 + 0.38 \\log_2 0.38)$\n",
    "$H(Y | X) = 0.6 \\cdot (-0.9 \\log_2 0.9 - 0.1 \\log_2 0.1) + 0.4 \\cdot (-0.8 \\log_2 0.8 - 0.2 \\log_2 0.2)$\n",
    "\n",
    "   **Step 3:**  \n",
    "$I(X; Y) = H(Y) - H(Y | X)$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Index Coding**  \n",
    "1. **Q:**  \n",
    "   For a system with 4 clients and 4 messages, each client knows all messages except the one they request. Find the optimal number of transmissions.\n",
    "\n",
    "   **A:**  \n",
    "   Use **XOR-based** coding. Transmit:  \n",
    "$W_1 \\oplus W_2 \\oplus W_3 \\oplus W_4$\n",
    "   Only 1 transmission is required.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Network Coding**  \n",
    "1. **Q:**  \n",
    "   In a butterfly network, if sources $A = 1$ and $B = 0$, compute the transmitted coded message and the values decoded at both sinks.\n",
    "\n",
    "   **A:**  \n",
    "   Transmit: $X = A \\oplus B = 1$.  \n",
    "   Sinks decode:  \n",
    "$A = X \\oplus B = 1, \\quad B = X \\oplus A = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Coded Caching**  \n",
    "1. **Q:**  \n",
    "   In a coded caching system with $N = 6$, $K = 3$, and $M = 2$, calculate the transmission load during the delivery phase.\n",
    "\n",
    "   **A:**  \n",
    "$L = \\frac{N(1 - M/N)}{1 + KM/N} = \\frac{6(1 - 2/6)}{1 + 3(2/6)} = 2$\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Gambling (after 10 Gains)**  \n",
    "\n",
    "1. **Q:**  \n",
    "   After making 10 gains, you want to maximize your long-term wealth by reinvesting a portion of your capital on each bet. Suppose the gain probability is $p = 0.6$ and the odds are $b = 1.8$. Calculate the **expected long-term growth rate** if you follow the optimal strategy.\n",
    "\n",
    "   **A:**  \n",
    "   The **expected growth rate** $G$ is given by:  \n",
    "$G = p \\log_2 (1 + bf^*) + (1-p) \\log_2 (1 - f^*)$\n",
    "\n",
    "   **Step 1:** Calculate $f^*$:  \n",
    "$f^* = \\frac{1.8 \\cdot 0.6 - 0.4}{1.8} = \\frac{1.08 - 0.4}{1.8} = 0.3778$\n",
    "\n",
    "   **Step 2:** Substitute into the growth rate formula:  \n",
    "$G = 0.6 \\log_2 (1 + 1.8 \\cdot 0.3778) + 0.4 \\log_2 (1 - 0.3778)$\n",
    "\n",
    "   Approximation yields:  \n",
    "$G \\approx 0.6 \\cdot 0.77 + 0.4 \\cdot (-0.59) \\approx 0.322$\n",
    "\n",
    "   The expected growth rate is approximately **0.322 bits** per bet.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **10. MAC or Broadcast Channel (Optimal Schemes)**  \n",
    "1. **Q:**  \n",
    "   For a broadcast channel with $P = 10$ and noise levels $N_1 = 1$, $N_2 = 4$, find the achievable rates.\n",
    "\n",
    "   **A:**  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2(1 + 10), \\quad R_2 \\leq \\frac{1}{2} \\log_2 \\left( \\frac{N_1}{N_2} \\right) = 0.5$\n",
    "\n",
    "---\n",
    "\n",
    "### **11. EAP (Picking Dual Subset of Numbers)**  \n",
    "1. **Q:**  \n",
    "   A random variable has entropy $H(X) = 2$. For $n = 10$, how many sequences are needed to capture 99% of the total probability?\n",
    "\n",
    "   **A:**  \n",
    "$\\text{Probability per sequence} = 2^{-nH(X)} = 2^{-20}, \\quad m \\cdot 2^{-20} \\geq 0.99$\n",
    "   Solving:  \n",
    "$m \\geq 0.99 \\cdot 2^{20} \\approx 1.04 \\times 10^6$\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Coded MapReduce**  \n",
    "1. **Q:**  \n",
    "   In a system with 5 mappers and 4 reducers, calculate the communication cost reduction using coded MapReduce.\n",
    "\n",
    "   **A:**  \n",
    "$R = \\frac{1}{r} = \\frac{1}{4}$\n",
    "\n",
    "---\n",
    "\n",
    "These advanced problems provide a thorough challenge across **Information Theory** topics, requiring deep mathematical understanding and application of key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649802ad-a799-4cf9-8ac4-d221ee55781e",
   "metadata": {},
   "source": [
    "## Exercices 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaba7bc-5101-40bf-942d-994eb682fcb0",
   "metadata": {},
   "source": [
    "### **Advanced Information Theory Q&A – Difficult Mathematical Problems**  \n",
    "*(Based on Elements of Information Theory, 2nd Edition by Cover & Thomas)*\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Capacity Regions**\n",
    "1. **Q:**  \n",
    "   Consider a two-user MAC where user 1 transmits with power $P_1 = 4$ and user 2 with $P_2 = 16$. The noise variance is $N = 1$. Derive the capacity region equations and find a rate pair $(R_1, R_2)$ where $R_1 = 1$.  \n",
    "\n",
    "   **A:**  \n",
    "   The capacity region equations are:  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1}{N} \\right), \\quad R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_2}{N} \\right), \\quad R_1 + R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1 + P_2}{N} \\right)$\n",
    "\n",
    "   Substituting values:  \n",
    "$R_1 \\leq 1, \\quad R_2 \\leq 2, \\quad R_1 + R_2 \\leq 1.8$\n",
    "\n",
    "   For $R_1 = 1$, $R_2$ must satisfy:  \n",
    "$R_2 \\leq 0.8$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Markov Chains**  \n",
    "1. **Q:**  \n",
    "   For a Markov chain with the transition matrix:  \n",
    "$P = \\begin{bmatrix} 0.6 & 0.4 \\\\ 0.3 & 0.7 \\end{bmatrix},$\n",
    "   calculate the **second-order entropy rate**, assuming the chain starts in the stationary distribution.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Find the stationary distribution $\\pi$:  \n",
    "$\\pi_1 = 0.6\\pi_1 + 0.3\\pi_2, \\quad \\pi_1 + \\pi_2 = 1 \\quad \\Rightarrow \\quad \\pi = \\left( \\frac{3}{7}, \\frac{4}{7} \\right)$\n",
    "\n",
    "   **Step 2:** Calculate the second-order joint entropy:  \n",
    "$H(X_1, X_2) = \\sum_{i, j} \\pi(i) P_{ij} \\log_2 \\frac{1}{P_{ij}}$\n",
    "\n",
    "   Substituting values:  \n",
    "$H(X_1, X_2) = \\frac{3}{7} \\left( 0.6 \\log_2 \\frac{1}{0.6} + 0.4 \\log_2 \\frac{1}{0.4} \\right) + \\frac{4}{7} \\left( 0.3 \\log_2 \\frac{1}{0.3} + 0.7 \\log_2 \\frac{1}{0.7} \\right)$\n",
    "\n",
    "   Finally, calculate the **entropy rate** using:  \n",
    "$H(X) = H(X_1, X_2) - H(X_1)$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Maximization of Entropy**  \n",
    "1. **Q:**  \n",
    "   Prove that the entropy of a continuous random variable $X$ is maximized when $X \\sim \\mathcal{N}(0, \\sigma^2)$, by using the calculus of variations.\n",
    "\n",
    "   **A:**  \n",
    "   The functional form of entropy is:  \n",
    "$h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) \\, dx$\n",
    "\n",
    "   Applying the Euler-Lagrange equation with the constraint $\\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx = \\sigma^2$ leads to the solution:  \n",
    "$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{x^2}{2 \\sigma^2}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Capacities of Different Channels**  \n",
    "1. **Q:**  \n",
    "   Calculate the capacity of an AWGN channel with bandwidth $B = 5 \\, \\text{MHz}$, signal power $P = 0.1 \\, \\text{W}$, and noise power spectral density $N_0 = 10^{-8} \\, \\text{W/Hz}$.\n",
    "\n",
    "   **A:**  \n",
    "   Capacity is given by:  \n",
    "$C = B \\log_2 \\left( 1 + \\frac{P}{N_0 B} \\right)$\n",
    "\n",
    "   Substituting values:  \n",
    "$C = 5 \\times 10^6 \\log_2 \\left( 1 + \\frac{0.1}{5 \\times 10^{-8}} \\right) = 5 \\times 10^6 \\log_2 (2001)$\n",
    "\n",
    "   Approximation:  \n",
    "$C \\approx 5 \\times 10^6 \\times 10.97 = 54.85 \\, \\text{Mbps}$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Calculate Entropy of Channels**  \n",
    "1. **Q:**  \n",
    "   A channel has input $X$ and output $Y$ with the following joint probability table:  \n",
    "$P(X, Y) = \\begin{bmatrix} 0.3 & 0.2 \\\\ 0.1 & 0.4 \\end{bmatrix}$\n",
    "   Calculate $I(X; Y)$.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Calculate $H(X)$ and $H(Y)$:  \n",
    "$H(X) = - (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1, \\quad H(Y) = - (0.4 \\log_2 0.4 + 0.6 \\log_2 0.6)$\n",
    "\n",
    "   **Step 2:** Calculate $H(X, Y)$:  \n",
    "$H(X, Y) = -\\sum_{i,j} P(x_i, y_j) \\log_2 P(x_i, y_j)$\n",
    "\n",
    "   **Step 3:**  \n",
    "$I(X; Y) = H(X) + H(Y) - H(X, Y)$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Index Coding**  \n",
    "1. **Q:**  \n",
    "   For 5 clients and 5 messages where each client is missing only the message they want, calculate the optimal number of coded transmissions.\n",
    "\n",
    "   **A:**  \n",
    "   Use a single XOR-coded transmission:  \n",
    "$W_1 \\oplus W_2 \\oplus W_3 \\oplus W_4 \\oplus W_5$\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Network Coding**  \n",
    "1. **Q:**  \n",
    "   In a network with two sources $A$ and $B$, transmit $X = A \\oplus B$. If $A = 1$ and $B = 1$, what is received and decoded at each sink?\n",
    "\n",
    "   **A:**  \n",
    "$X = 1 \\oplus 1 = 0$\n",
    "\n",
    "   Sinks receive $X = 0$ and decode:  \n",
    "$A = X \\oplus B = 0 \\oplus 1 = 1, \\quad B = X \\oplus A = 0 \\oplus 1 = 1$\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Coded Caching**  \n",
    "1. **Q:**  \n",
    "   For $N = 8$, $K = 4$, and $M = 2$, calculate the transmission load.\n",
    "\n",
    "   **A:**  \n",
    "$L = \\frac{N(1 - M/N)}{1 + KM/N} = \\frac{8(1 - 2/8)}{1 + 4(2/8)} = \\frac{6}{2} = 3$\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Gambling (after 10 Gains)**  \n",
    "\n",
    "1. **Q:**  \n",
    "   Suppose you use a suboptimal betting strategy, placing a constant fraction $f = 0.5$ of your wealth on each bet. If the true optimal $f^* = 0.3778$, determine the relative difference in long-term growth rate between the optimal and suboptimal strategies.\n",
    "\n",
    "   **A:**  \n",
    "   **Step 1:** Calculate the suboptimal growth rate:  \n",
    "$G_{\\text{suboptimal}} = 0.6 \\log_2 (1 + 0.9) + 0.4 \\log_2 (0.5)$\n",
    "   Approximation:  \n",
    "$G_{\\text{suboptimal}} \\approx 0.6 \\cdot 0.92 + 0.4 \\cdot (-1) = 0.152$\n",
    "\n",
    "   **Step 2:** Compare with optimal growth rate $G^* = 0.322$:  \n",
    "$\\Delta G = G^* - G_{\\text{suboptimal}} = 0.322 - 0.152 = 0.17$\n",
    "\n",
    "   The **relative difference** is:  \n",
    "$\\frac{\\Delta G}{G^*} \\approx \\frac{0.17}{0.322} \\approx 0.53 \\, (53\\%)$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **10. MAC or Broadcast Channel (Optimal Schemes)**  \n",
    "1. **Q:**  \n",
    "   In a broadcast channel, the transmitter can send two messages $M_1$ and $M_2$ to two users with noise levels $N_1 = 1$ and $N_2 = 4$, respectively. The power constraint is $P = 10$. Find the achievable rate pair using **superposition coding**.\n",
    "\n",
    "   **A:**  \n",
    "   - **Step 1:** Assign power allocations $P_1$ and $P_2$ with $P_1 + P_2 = P$.  \n",
    "   - **Step 2:** Calculate rates:  \n",
    "     For user 1 (stronger channel):  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1 + P_2}{N_1} \\right) = \\frac{1}{2} \\log_2 (1 + 10)$\n",
    "     For user 2 (weaker channel):  \n",
    "$R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_2}{N_2} \\right)$\n",
    "\n",
    "   - **Step 3:** Choose $P_1 = 6$, $P_2 = 4$:  \n",
    "$R_1 = \\frac{1}{2} \\log_2 (1 + 10) \\approx 1.73, \\quad R_2 = \\frac{1}{2} \\log_2 (1 + 1) = 0.5$\n",
    "\n",
    "---\n",
    "\n",
    "2. **Q:**  \n",
    "   In a MAC with users transmitting powers $P_1 = 5$ and $P_2 = 15$, and noise variance $N = 1$, what is the achievable sum rate using **successive decoding**?\n",
    "\n",
    "   **A:**  \n",
    "   - **Step 1:** Calculate individual rates:  \n",
    "$R_1 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1}{N} \\right) = \\frac{1}{2} \\log_2 (6), \\quad R_2 \\leq \\frac{1}{2} \\log_2 (16)$\n",
    "\n",
    "   - **Step 2:** Calculate sum-rate:  \n",
    "$R_1 + R_2 \\leq \\frac{1}{2} \\log_2 \\left( 1 + \\frac{P_1 + P_2}{N} \\right) = \\frac{1}{2} \\log_2 (21) \\approx 2.14 \\, \\text{bits/symbol}$\n",
    "\n",
    "---\n",
    "\n",
    "### **11. EAP (Picking Dual Subset of Numbers)**  \n",
    "1. **Q:**  \n",
    "   A random variable has entropy $H(X) = 1.5$. For $n = 20$, how many sequences are needed to cover 95% of the probability?\n",
    "\n",
    "   **A:**  \n",
    "   Probability of each typical sequence:  \n",
    "$P(x^n) = 2^{-nH(X)} = 2^{-30}$\n",
    "\n",
    "   Solve:  \n",
    "$m \\cdot 2^{-30} \\geq 0.95 \\quad \\Rightarrow \\quad m \\geq 0.95 \\cdot 2^{30} \\approx 1.02 \\times 10^9$\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Coded MapReduce**  \n",
    "1. **Q:**  \n",
    "   In a coded MapReduce setup, there are 6 mappers and 3 reducers. Each mapper generates intermediate data needed by all reducers. How many transmissions are required without and with coding?\n",
    "\n",
    "   **A:**  \n",
    "   - **Without coding:** Each mapper sends all data to each reducer. Total transmissions:  \n",
    "$6 \\, \\text{mappers} \\times 3 \\, \\text{reducers} = 18$\n",
    "\n",
    "   - **With coding:** Use a coded transmission strategy, where each mapper encodes data and sends only once. Total transmissions:  \n",
    "$\\frac{1}{r} \\cdot \\text{number of intermediate blocks} = \\frac{1}{3} \\times 6 = 2 \\, \\text{transmissions per reducer}$\n",
    "\n",
    "   - Total transmissions with coding:  \n",
    "     $6$.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Q:**  \n",
    "   If each reducer needs access to $3$ pieces of data and each mapper can encode $2$ pieces of data together, find the minimum number of coded transmissions required.\n",
    "\n",
    "   **A:**  \n",
    "   - Data pieces per reducer = 3.  \n",
    "   - Each mapper can combine 2 pieces, reducing the number of transmissions:  \n",
    "$\\lceil 3 / 2 \\rceil = 2 \\, \\text{coded transmissions per reducer}$\n",
    "\n",
    "   Total transmissions across all reducers:  \n",
    "$3 \\, \\text{reducers} \\times 2 = 6$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e356d-ab74-45b5-862f-909234f124d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
